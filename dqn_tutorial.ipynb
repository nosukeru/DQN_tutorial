{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for DQN (Deep Q-Network)\n",
    "\n",
    "This is a jupyter tutorial for DQN (Deep Q-Network), a model-based algorithm for reinforcement learning.\n",
    "\n",
    "## Requirements\n",
    "- Python3.5 or higher\n",
    "- Pip for Python3\n",
    "- GPU environment\n",
    "\n",
    "## Step1. Setup environment\n",
    "In this step, we're going to install required modules. We use GPU environment (below we call remote) for training model, and need to share the same environment (i.e. installed modules) between local and remote. In this purpose, we use `pipenv`, a tool for package-managing and virtual environment.\n",
    "\n",
    "### Step1.1. Setup pipenv for local\n",
    "Run below commands in your local environment.\n",
    "\n",
    "```\n",
    "$ pip3 install pipenv --user\n",
    "$ cd {project-directory}\n",
    "$ pipenv --python3\n",
    "```\n",
    "\n",
    "Install required modules.\n",
    "\n",
    "```\n",
    "$ pipenv install numpy\n",
    "$ pipenv install torch torchvision\n",
    "$ pipenv install gym gym[atari]\n",
    "$ pipenv install python-dotenv\n",
    "```\n",
    "\n",
    "## Step2. Implementation\n",
    "Now let's start implementation.\n",
    "\n",
    "Note: Code below is revised for jupyter notebook (for module import/usage). Complete version is found in https://github.com/nosukeru/DQN_tutorial \n",
    "\n",
    "### Step2.1. Configuration\n",
    "First, we setup some configuration for switching local/remote environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('.env')\n",
    "isLocal = (os.environ.get('ENV') == 'local')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And edit .env file.\n",
    "\n",
    "```\n",
    "# .env\n",
    "ENV=local\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.2. Implement Q-Network\n",
    "Q-Network is a deep neural network model approximating Q-function, which represents potential total reward for current state and action under policy $\\pi(a|s)$:\n",
    "\n",
    "$$ Q^\\pi(s, a) = r(s, a) + E_{s' \\sim P(s'|s, a)} [V(s')] $$\n",
    "$$ V^\\pi(s) = E_{a \\sim \\pi(a|s)} [Q(s, a)] $$\n",
    "\n",
    "When policy $\\pi$ is optimal, the following Bellman equation holds:\n",
    "\n",
    "$$ Q^\\ast(s, a) = r(s, a) + E_{s' \\sim P(s'|s, a)} [\\max_{a'} Q^\\ast(s', a')] $$\n",
    "\n",
    "Our objective is to approximate optimal Q-function by deep neural network.\n",
    "Here, we design Q-Network to take $s$ as an input and output a vector of Q-values corresponding to each $a$, because of calculation efficiency. Note that this is possible only when action space is countable and independent on state.\n",
    "\n",
    "Below is an implementation of Q-Network using PyTorch. Input is a Torch tensor of stacked preprocessed frames of size (batch, 4, 84, 84) (detailed in later), and output is also a Torch tensor of Q-values corresponding actions. Architecture is following that of the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models.py\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Q-Network model\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, nAction):\n",
    "        # nAction: number of action (depends on gym environment)\n",
    "\n",
    "        super(QNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)  # (4, 84, 84) -> (32, 20, 20)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)  # (32, 20, 20) -> (64, 9, 9)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)  # (64, 9, 9) -> (64, 7, 7)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, nAction)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # run forward propagation\n",
    "        # x: state (4 stacked grayscale frames of size 84x84)\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc1(x.view(x.size(0), -1)))  # flatten\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3. Implement Policy\n",
    "\n",
    "And when Q-function is optimal, best policy for given state $s$ is to select an action $a$ which maximizes Q-value:\n",
    "\n",
    "$$ \\pi(a|s) = \\underset{a}{\\operatorname{argmax}} Q(s, a) $$\n",
    "\n",
    "To facilitate exploration, we use epsilon greedy in which agent takes action randomly with probability $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents.py\n",
    "\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# agent (or policy) model\n",
    "class Agent(object):\n",
    "    def __init__(self, nAction, Q):\n",
    "        # nAction: number of action (depends on gym environment)\n",
    "        # Q: policy network\n",
    "\n",
    "        self.nAction = nAction\n",
    "        self.Q = Q\n",
    "\n",
    "    def getAction(self, state, eps):\n",
    "        # calc best action for given state\n",
    "        # state: state (4 stacked grayscale frames of size 84x84)\n",
    "        # eps: value for epsilon greeedy\n",
    "\n",
    "        var = Variable(state)\n",
    "        if not isLocal:\n",
    "            var = var.cuda()\n",
    "\n",
    "        # action with max Q value (q for value and argq for index)\n",
    "        q, argq = self.Q(var).max(1) \n",
    "\n",
    "        # epsilon greedy\n",
    "        probs = np.full(self.nAction, eps / self.nAction, np.float32)\n",
    "        probs[argq[0]] += 1 - eps\n",
    "        return np.random.choice(np.arange(self.nAction), p=probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.4. Implement Trainer\n",
    "When it came to training network, there are some techniques to improve its stability and efficiency:\n",
    "\n",
    "#### ReplayBuffer\n",
    "Saving trajectories to a memory and training network by samples randomly taken from this buffer has some benefits:\n",
    "\n",
    "- enable batch training\n",
    "- reduce correlation between experiences\n",
    "- avoid forgetting previous experiences\n",
    "\n",
    "#### Frozen target network\n",
    "The output of Q-Network is needed to calculate policy, so updating Q-Network always results to changing policy, which causes learning instability. In order to improve stability, We can use the copy of Q-Network for calculating policy, whose weights are fixed for a while.\n",
    "\n",
    "In these techniques in mind, we can use the following mean squared loss as a learning objective:\n",
    "\n",
    "$$ \\sum_{i \\in batch} \\frac{1}{2} \\{ Q(s_i, a_i) - (r_i + \\max_{a'} Q_{target}(s'_i, a')) \\} ^2 $$\n",
    "\n",
    "where batch (s, a, r, s') = (state, action, reward, nextState) is sampled from ReplayBuffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainers.py\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# utility class for training Q-Network\n",
    "class Trainer(object):\n",
    "    def __init__(self, Q, QTarget, opt, args):\n",
    "        # Q: Q-Network\n",
    "        # QTarget: Target Q-Network\n",
    "        # opt: optimizer\n",
    "\n",
    "        self.Q = Q\n",
    "        self.QTarget = QTarget\n",
    "        self.opt = opt\n",
    "\n",
    "        self.gamma = args.gamma\n",
    "        self.lossFunc = nn.MSELoss()\n",
    "\n",
    "    def update(self, batch):\n",
    "        # update model for given batch\n",
    "        # batch: training batch of (state, action, reward, nextState)\n",
    "\n",
    "        # extract training batch\n",
    "        stateBatch = Variable(torch.cat([step.state for step in batch], 0))\n",
    "        actionBatch = torch.LongTensor([step.action for step in batch])\n",
    "        rewardBatch = torch.Tensor([step.reward for step in batch])\n",
    "        nextStateBatch = Variable(torch.cat([step.nextState for step in batch], 0))\n",
    "\n",
    "        if not isLocal:\n",
    "            stateBatch = stateBatch.cuda()\n",
    "            actionBatch = actionBatch.cuda()\n",
    "            rewardBatch = rewardBatch.cuda()\n",
    "            nextStateBatch = nextStateBatch.cuda()\n",
    "\n",
    "        # calc values for update model\n",
    "        qValue = self.Q(stateBatch).gather(1, actionBatch.unsqueeze(1)).squeeze(1)  # Q(s, a)\n",
    "        qTarget = rewardBatch + self.QTarget(nextStateBatch).detach().max(1)[0] * self.gamma  # r + γmaxQ(s', a')\n",
    "\n",
    "        L = self.lossFunc(qValue, qTarget)  # loss to equalize Q(s) and r + γmaxQ(s', a')\n",
    "        self.opt.zero_grad()\n",
    "        L.backward()\n",
    "        self.opt.step()  # train for one batch step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.5. Implement ReplayBuffer\n",
    "Below is a code for naive implementation of ReplayBuffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "import random\n",
    "from typing import NamedTuple\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "# one step of interaction with environment\n",
    "class Step(NamedTuple):\n",
    "    state: torch.Tensor\n",
    "    action: int\n",
    "    reward: float\n",
    "    nextState: torch.Tensor\n",
    "\n",
    "\n",
    "# replay buffer\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        # capacity: max size of replay buffer\n",
    "\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.index = 0\n",
    "\n",
    "    def push(self, step):\n",
    "        # add  a step to buffer\n",
    "        # step: one step of interaction\n",
    "\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(step)\n",
    "        else:\n",
    "            self.memory[self.index] = step\n",
    "\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "\n",
    "    def sample(self, size):\n",
    "        # collect batch of given size\n",
    "        # size: batch size\n",
    "\n",
    "        return random.sample(self.memory, size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.6. Implement preprocessing of frames\n",
    "We also need some preprocessing of raw frames:\n",
    "\n",
    "- resizing and trimming original frame of size (210, 160) into (84, 84) for less memory size\n",
    "- grayscaling for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "def preprocess(x):\n",
    "    # preprocess frame\n",
    "    # x: a frame of size 210x160\n",
    "    \n",
    "    # resize, grayscale and convert to tensor\n",
    "    transform = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.Resize(84),\n",
    "        T.Grayscale(),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "\n",
    "    return transform(x[50:, :, :]).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.7. Implement training loop\n",
    "Finally we're ready to implement main training loop, but there're still some points to consider.\n",
    "\n",
    "#### State as stacked frames\n",
    "To capture the movement betweeen frames, it is beneficial to stack some frames as a state, instead of a single frame.\n",
    "\n",
    "#### Frame skip\n",
    "Deciding action per each frame may be too frequent, because even expert (human player) can't decide action so quickly. In addition, state scarcely changes in adjacent frames and too many decision can result in training instability, so it can be helpful to skip some frames and take the same action as previous decision for these frames.\n",
    "\n",
    "#### Training skip\n",
    "Similar to frame skip, training per each frame can be too frequent and we'll train model one for some frames.\n",
    "\n",
    "#### Initial waiting\n",
    "It is better not to start training until enough experiences are saved in ReplayBuffer to prevent overfitting.\n",
    "\n",
    "Below code is a full implementation of training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1\n",
      "\n",
      "  reward 1.000000\n",
      "\n",
      "  model saved\n",
      "episode: 2\n",
      "\n",
      "  reward 2.000000\n",
      "\n",
      "episode: 3\n",
      "\n",
      "  reward 0.000000\n",
      "\n",
      "episode: 4\n",
      "\n",
      "  reward 2.000000\n",
      "\n",
      "episode: 5\n",
      "\n",
      "  reward 0.000000\n",
      "\n",
      "episode: 6\n",
      "\n",
      "  reward 0.000000\n",
      "\n",
      "episode: 7\n",
      "\n",
      "  reward 0.000000\n",
      "\n",
      "episode: 8\n",
      "\n",
      "  reward 0.000000\n",
      "\n",
      "episode: 9\n",
      "\n"
     ]
    },
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m_ctypes/callbacks.c\u001b[0m in \u001b[0;36m'calling callback function'\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/DQN_tutorial-I7qmx_lw/lib/python3.7/site-packages/pyglet/libs/darwin/cocoapy/runtime.py\u001b[0m in \u001b[0;36mobjc_method\u001b[0;34m(objc_self, objc_cmd, *args)\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mpy_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mObjCInstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjc_self\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m                 \u001b[0mpy_self\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjc_cmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjc_cmd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_method_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mObjCClass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/DQN_tutorial-I7qmx_lw/lib/python3.7/site-packages/pyglet/libs/darwin/cocoapy/runtime.py\u001b[0m in \u001b[0;36mconvert_method_arguments\u001b[0;34m(encoding, args)\u001b[0m\n\u001b[1;32m    998\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb'@'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m             \u001b[0mnew_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mObjCInstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb'#'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0mnew_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mObjCClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/DQN_tutorial-I7qmx_lw/lib/python3.7/site-packages/pyglet/libs/darwin/cocoapy/runtime.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, object_ptr)\u001b[0m\n\u001b[1;32m    919\u001b[0m         for the given object_ptr which should be an Objective-C id.\"\"\"\n\u001b[1;32m    920\u001b[0m         \u001b[0;31m# Make sure that object_ptr is wrapped in a c_void_p.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_ptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m             \u001b[0mobject_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def run(args):\n",
    "    # setup\n",
    "    env = gym.make('Breakout-v0')\n",
    "\n",
    "    nAction = env.action_space.n\n",
    "    buffer = ReplayBuffer(args.buffer_size)\n",
    "\n",
    "    Q = QNet(nAction)\n",
    "    QTarget = QNet(nAction)\n",
    "\n",
    "    if args.model_path is not None:\n",
    "        state_dict = None\n",
    "        if isLocal:\n",
    "            state_dict = torch.load(args.model_path, map_location='cpu')\n",
    "        else:\n",
    "            state_dict = torch.load(args.model_path)\n",
    "\n",
    "        Q.load_state_dict(state_dict)\n",
    "        QTarget.load_state_dict(state_dict)\n",
    "\n",
    "    Q.train()\n",
    "    QTarget.eval()\n",
    "\n",
    "    if not isLocal:\n",
    "        Q = Q.cuda()\n",
    "        QTarget = QTarget.cuda()\n",
    "\n",
    "    opt = optim.Adam(Q.parameters(), lr=args.lr)\n",
    "\n",
    "    agent = Agent(nAction, Q)\n",
    "    trainer = Trainer(Q, QTarget, opt, args)\n",
    "\n",
    "    t = 0\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    for episode in range(args.episode):\n",
    "        print(\"episode: %d\\n\" % (episode + 1))\n",
    "\n",
    "        observation = env.reset()\n",
    "        state = torch.cat([preprocess(observation)] * 4, 1)  # initial state\n",
    "        sum_reward = 0\n",
    "\n",
    "        # Exploration loop\n",
    "        done = False\n",
    "        while not done:\n",
    "            if isLocal:\n",
    "                env.render()\n",
    "\n",
    "            # frame skip\n",
    "            if t % args.frame_skip == 0:\n",
    "                action = agent.getAction(state, args.eps)\n",
    "\n",
    "            # take action and calc next state\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            nextState = torch.cat([state.narrow(1, 1, 3), preprocess(observation)], 1)\n",
    "\n",
    "            buffer.push(Step(state, action, reward, nextState))\n",
    "            state = nextState\n",
    "            sum_reward += reward\n",
    "            t += 1\n",
    "\n",
    "            # initial waiting\n",
    "            if t < args.initial_wait:\n",
    "                continue\n",
    "\n",
    "            # update model\n",
    "            if t % args.train_freq == 0:\n",
    "                batch = buffer.sample(args.batch)\n",
    "                trainer.update(batch)\n",
    "\n",
    "            # update target\n",
    "            if t % args.target_update_freq == 0:\n",
    "                QTarget.load_state_dict(Q.state_dict())\n",
    "\n",
    "        print(\"  reward %f\\n\" % sum_reward)\n",
    "\n",
    "        if episode % args.snapshot_freq == 0:\n",
    "            torch.save(Q.state_dict(), \"results/%d.pth\" % (episode + 1))\n",
    "            print(\"  model saved\")\n",
    "\n",
    "    torch.save(Q.state_dict(), \"results/model.pth\")\n",
    "    env.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--episode', type=int, default=12000)\n",
    "    parser.add_argument('--buffer_size', type=int, default=400000)\n",
    "    parser.add_argument('--train_freq', type=int, default=4)\n",
    "    parser.add_argument('--initial_wait', type=int, default=20000)\n",
    "    parser.add_argument('--batch', type=int, default=32)\n",
    "    parser.add_argument('--target_update_freq', type=int, default=10000)\n",
    "    parser.add_argument('--lr', type=float, default=0.0003)\n",
    "    parser.add_argument('--frame_skip', type=int, default=4)\n",
    "    parser.add_argument('--snapshot_freq', type=int, default=1000)\n",
    "    parser.add_argument('--eps', type=float, default=0.05)\n",
    "    parser.add_argument('--gamma', type=float, default=0.99)\n",
    "    parser.add_argument('--model_path', type=str)\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    run(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Training in remote environment\n",
    "Once you can confirm that training loop runs correctly in local environment, it's time to train model in remote environment with GPU.\n",
    "\n",
    "### Step 3.1. Bring code in remote\n",
    "You can use your own source repository, or run the following command:\n",
    "\n",
    "```\n",
    "$ \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
